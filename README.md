# üé∂ Real‚ÄëTime Emotion‚ÄëAware Music Generator (Full README)

Welcome to the emotional command center of your AI music universe. This repository fuses **computer vision**, **NLP emotion detection**, and a **GAN-powered symbolic music generator** into one seamless pipeline that reacts to your expressions *and* your text ‚Äî crafting original MIDI melodies in real time.

Camera ‚Üí Text ‚Üí Emotion ‚Üí GAN ‚Üí MIDI ‚Üí Magic.

This README gives you:
- A full breakdown of every module in the repo
- Architecture diagrams (ASCII-style)
- How the GAN, Emotion Discriminator, and Dataset Pipeline fit together
- Installation & running guide
- A summary of requirements based on your `requirements.txt` ÓàÄfileciteÓàÇturn0file4ÓàÅ

---

# üöÄ 1. Project Overview
This system is a **multimodal emotional soundtrack engine**. It listens to your face, reads your text, and generates MIDI that reflects those emotions.

It brings together three major subsystems:
1. **Real‚ÄëTime Camera Emotion Detection** ‚Äî Facial expressions ‚Üí emotion
2. **Text‚ÄëBased Emotion Classification** ‚Äî Typed messages ‚Üí emotion
3. **MELO‚ÄëGAN (WGAN‚ÄëGP)** ‚Äî Emotion‚Äëconditioned symbolic music generation

Each of them is independently testable but fully integrated via `app.py`.

---

# üß© 2. Repository Structure
Below is a clean summary of the key modules.

## üé• 2.1 camera.py ‚Äî Real‚ÄëTime Facial Emotion Detection
Handles live webcam emotion detection using:
- OpenCV SSD Face Detector
- Mini‚ÄëXception CNN trained on FER‚Äë2013

Emotion mapping (7 ‚Üí 4 classes):
```
angry, fear ‚Üí angry
sad, disgust ‚Üí sad
happy, surprise ‚Üí happy
neutral ‚Üí calm
```

Outputs:
- Global variable: `current_emotion`
- Video stream via `generate_frames()`

Source reference: camera.py details (from provided docs) ÓàÄfileciteÓàÇturn0file0ÓàÅ

---

## üìù 2.2 text.py ‚Äî Text‚ÄëBased Emotion Classification
Uses the **SamLowe/roberta-base-go_emotions** model to classify 28 fine-grained emotions ‚Üí 4 macro categories.

Exposes:
```python
def predict_emotion(text):
    return mapped_emotion
```

This powers the `/get_text_emotion` endpoint.

Source reference: text.py section in your docs ÓàÄfileciteÓàÇturn0file0ÓàÅ

---

## üß† 2.3 MELO‚ÄëGAN ‚Äî Emotion‚ÄëDriven Music Generator
This subsystem includes:
- WGAN‚ÄëGP Generator & Critic
- Feature Encoder for conditioning vectors
- Emotion Discriminator (ED) for emotion supervision

Full architecture & file-level explanations come from your MELO‚ÄëGAN docs ÓàÄfileciteÓàÇturn0file1ÓàÅ.

### Components
- `feature_encoder.py` ‚Äî Converts numeric conditioning ‚Üí latent embedding
- `models.py` ‚Äî GAN generator & critic
- `train_gan.py` ‚Äî Full training pipeline
- `test_gan.py` ‚Äî Generates samples based on chosen emotion

Outputs:
- Pitch, velocity, duration, step ‚Üí MIDI
- Emotion‚Äëconditioned melodies using scales, BPM, and instrument rules

---

## üéõ 2.4 Emotion Discriminator (ED)
Used during GAN training for emotional correctness.

Implements:
- Conv1D notes encoder
- MLP classifier
- Supports latent and notes input

Your ED documentation confirms:
- Proper shape handling
- Correct training loop
- Correct checkpoint system

Source: ED README ÓàÄfileciteÓàÇturn0file2ÓàÅ

---

## üéº 2.5 Dataset Pipeline ‚Äî EMOPIA + VGMIDI Unified Dataset
Your dataset pipeline is fully automated and model-ready.

Stages:
1. Label merging ‚Üí unified 4‚Äëemotion taxonomy
2. Manifest building
3. MIDI preprocessing ‚Üí (512 √ó 4 note matrices)
4. Train/val/test splitting with feature scaling

Source: Dataset Pipeline README ÓàÄfileciteÓàÇturn0file3ÓàÅ

---

# üß¨ 3. System Architecture
```
                +---------------------+
 Webcam ------> |   camera.py         |
                | SSD + MiniXception  |
                | current_emotion     |
                +----------+----------+
                           |
                           v
                +---------------------+
 User Text ---> |    text.py          |
                | Transformer Emotion |
                +----------+----------+
                           |
                           v
                +------------------------------+
                |            app.py            |
                |  - Flask Backend             |
                |  - GAN Conditioning          |
                |  - Music Theory Engine       |
                |  - MIDI Generation           |
                +------------------------------+
                           |
                           v
                   Generated MIDI üéµ
```

---

# üéº 4. Emotion ‚Üí Music Mapping
| Emotion | Scales | BPM Range | Instruments |
|---------|--------|------------|-------------|
| **happy** | major, lydian, pentatonic | 120‚Äì150 | bright piano, celesta |
| **sad** | minor, dorian | 60‚Äì85 | harp, nylon guitar |
| **angry** | blues, minor | 140‚Äì170 | electric guitar, accordion |
| **calm** | major, lydian | 70‚Äì95 | vibraphone, music box |

These rules ensure that even without AI magic, melodies have the correct emotional feel.

---

# üß™ 5. API Endpoints (Flask)
### **GET /**
Loads the UI.

### **GET /video_feed**
Live MJPEG webcam stream.

### **GET /get_camera_emotion**
Returns the latest facial emotion.

### **POST /get_text_emotion**
Returns text‚Äëbased emotion.

### **POST /generate**
Runs the entire pipeline ‚Üí returns a MIDI file.

---

# ‚öôÔ∏è 6. Installation & Setup
### 1. Install Dependencies
Your `requirements.txt` includes everything required for:
- Deep learning: PyTorch, TensorFlow
- CV: OpenCV, RetinaFace, MTCNN
- NLP: Transformers, tokenizers
- Music: pretty_midi, mido
- Flask backend

Full file referenced here: requirements.txt ÓàÄfileciteÓàÇturn0file4ÓàÅ

Install:
```bash
pip install -r requirements.txt
```

### 2. Run the Server
```bash
python app.py
```

### 3. Open the App
Go to:
```
http://localhost:5000
```

You now have:
- Real-time emotion detection
- Text emotion classification
- GAN-driven MIDI generation

---

# üß∞ 7. How to Train the GAN (Optional)
```bash
python train_gan.py --config config/gan_config.yaml --ed_config config/ed_config.yaml
```

Generate samples:
```bash
python test_gan.py --emotion happy --samples 3 --out outputs/
```

---

# üì¶ 8. Dataset Usage
Load a processed .npz file:
```python
import numpy as np
x = np.load("dataset/processed/example.npz")
notes = x["notes"]
numeric = x["numeric_features"]
emotion = x["emotion"]
```

---

# üß† 9. Why This System Is Special
This repo brings together:
- Real‚Äëtime multimodal emotional AI
- Transformer NLP
- GAN-based symbolic music generation
- A fully normalized, unified emotion‚Äëlabeled dataset
- Flask backend for instant deployment

It‚Äôs modular, scalable, and production-ready.

---

# ü§ù 10. Contributions
- **K. PRATEEK REDDY**
- **M. KAUSHIK CHANDRA**
- **G. SHASHANK REDDY**

